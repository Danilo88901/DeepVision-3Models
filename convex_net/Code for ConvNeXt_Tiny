import zipfile
import os
import pandas as pd
from PIL import Image
from torch.optim.lr_scheduler import StepLR
from torch.utils.data import Dataset, DataLoader, random_split
import torchvision.transforms as transforms
import torch 
from torch import nn
import torch.nn.functional as F
from sklearn.metrics import accuracy_score, f1_score
from torchvision.models import convnext_tiny, ConvNeXt_Tiny_Weights
import matplotlib.pyplot as plt

# Custom dataset for Cassava
class CassavaDataset(Dataset):
    def __init__(self, csv_file, root_dir, transform=None):
        self.annotations = pd.read_csv(csv_file)
        self.root_dir = root_dir
        self.transform = transform

    def __len__(self):
        return len(self.annotations)

    def __getitem__(self, idx):
        img_path = os.path.join(self.root_dir, self.annotations.iloc[idx, 0])
        image = Image.open(img_path).convert("RGB")
        label = self.annotations.iloc[idx, 1]
        if self.transform:
            image = self.transform(image)
        return image, label

# Paths — adjust them for your setup
csv_train = '/content/cassava_data/train.csv'
img_dir = '/content/cassava_data/train_images'

# Basic data augmentations and preprocessing
transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.RandomRotation(15),
    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225])
])

# Dataset and loader
dataset = CassavaDataset(csv_file=csv_train, root_dir=img_dir, transform=transform)

# Example sanity check
for images, labels in DataLoader(dataset, batch_size=32):
    print(images.shape)  # Should be [32, 3, 224, 224]
    print(labels)
    break

# Train-validation split
train_size = int(0.9 * len(dataset))
val_size = len(dataset) - train_size
train_subset, val_subset = random_split(dataset, [train_size, val_size])

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# DataLoaders
train_loader = DataLoader(train_subset, batch_size=40, shuffle=True, num_workers=4, pin_memory=True)
val_loader = DataLoader(val_subset, batch_size=40, shuffle=False, num_workers=4, pin_memory=True)

# Load pre-trained ConvNeXt-Tiny model
weights = ConvNeXt_Tiny_Weights.DEFAULT
model = convnext_tiny(weights=weights)

# Replace the classification head for 5 classes
num_classes = 5
model.classifier[2] = nn.Linear(model.classifier[2].in_features, num_classes)

# Freeze first 60% of model parameters
def set_parameter_requires_grad(model, freeze_ratio=0.6):
    params = list(model.parameters())
    freeze_up_to = int(len(params) * freeze_ratio)
    for i, param in enumerate(params):
        param.requires_grad = False if i < freeze_up_to else True

set_parameter_requires_grad(model, freeze_ratio=0.6)

# Setup optimizer, loss, scheduler
model = model.to(device)

# Optionally compute class_weights beforehand if needed
class_weights = torch.tensor([1.0] * num_classes).to(device)  # Replace with actual weights if needed
loss_fn = nn.CrossEntropyLoss(weight=class_weights)

optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5, weight_decay=1e-3)
scheduler = StepLR(optimizer, step_size=5, gamma=0.1)

# Training loop
epochs = 12
train_losses, val_losses = [], []
train_accuracies, val_accuracies = [], []
train_f1s, val_f1s = [], []
best_val_f1 = 0.0

for epoch in range(epochs):
    model.train()
    train_loss, all_labels, all_preds = 0.0, [], []

    for batch, (X, y) in enumerate(train_loader):
        X, y = X.to(device), y.to(device)
        y_pred = model(X)
        loss = loss_fn(y_pred, y)

        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        y_pred_class = torch.argmax(y_pred, dim=1)
        all_labels.extend(y.cpu().numpy())
        all_preds.extend(y_pred_class.cpu().numpy())

        if batch % 300 == 0:
            print(f"Processed: {batch * len(X)}/{len(train_loader.dataset)} samples")

    train_loss /= len(train_loader)
    train_acc = accuracy_score(all_labels, all_preds)
    train_f1 = f1_score(all_labels, all_preds, average='macro')

    # Validation
    model.eval()
    val_loss, val_labels, val_preds = 0.0, [], []
    with torch.no_grad():
        for X_val, y_val in val_loader:
            X_val, y_val = X_val.to(device), y_val.to(device)
            y_pred_val = model(X_val)
            val_loss += loss_fn(y_pred_val, y_val).item()
            val_preds.extend(torch.argmax(y_pred_val, dim=1).cpu().numpy())
            val_labels.extend(y_val.cpu().numpy())

    val_loss /= len(val_loader)
    val_acc = accuracy_score(val_labels, val_preds)
    val_f1 = f1_score(val_labels, val_preds, average='macro')

    print(f"Epoch: {epoch + 1} | Train_loss: {train_loss:.4f} | Train_acc: {train_acc:.4f} | "
          f"Train_f1: {train_f1:.4f} | Val_loss: {val_loss:.4f} | Val_acc: {val_acc:.4f} | "
          f"Val_f1: {val_f1:.4f}")

    torch.save(model.state_dict(), f"model_epoch{epoch + 1}_valf1_{val_f1:.4f}.pth")
    if val_f1 > best_val_f1:
        best_val_f1 = val_f1
        torch.save(model.state_dict(), "best_model.pth")
        print("✅ Best model updated!")

    scheduler.step()
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)
    train_f1s.append(train_f1)
    val_f1s.append(val_f1)

# Visualization
epochs_range = range(1, epochs + 1)
plt.figure(figsize=(18, 5))

# Loss plot
plt.subplot(1, 3, 1)
plt.plot(epochs_range, train_losses, label='Train Loss', marker='o')
plt.plot(epochs_range, val_losses, label='Val Loss', marker='o')
plt.title('Loss per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Accuracy plot
plt.subplot(1, 3, 2)
plt.plot(epochs_range, train_accuracies, label='Train Accuracy', marker='o')
plt.plot(epochs_range, val_accuracies, label='Val Accuracy', marker='o')
plt.title('Accuracy per Epoch')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# F1 Score plot
plt.subplot(1, 3, 3)
plt.plot(epochs_range, train_f1s, label='Train F1 Score', marker='o')
plt.plot(epochs_range, val_f1s, label='Val F1 Score', marker='o')
plt.title('F1 Score per Epoch')
plt.xlabel('Epoch')
plt.ylabel('F1 Score')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()
